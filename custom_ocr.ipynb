{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpBLmUWUptTvDibe8yGWTL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpit-parejiya01/Text_Extraction_OCR/blob/main/custom_ocr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set image dimensions\n",
        "img_height = 32\n",
        "img_width = 128\n",
        "num_classes = 80  # Number of possible characters (adjust to your charset)\n",
        "\n",
        "# Build CNN Model for feature extraction\n",
        "def build_cnn(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Reshape((-1, 256))\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Build RNN for sequence prediction\n",
        "def build_rnn(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.LSTM(256, return_sequences=True),\n",
        "        layers.LSTM(256, return_sequences=True),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define the CTC Loss Function\n",
        "def ctc_loss_lambda_func(y_true, y_pred):\n",
        "    input_length = tf.math.reduce_sum(tf.ones_like(y_pred[:, :, 0]), axis=1)\n",
        "    label_length = tf.math.reduce_sum(tf.ones_like(y_true), axis=1)\n",
        "    return tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "\n",
        "# Build the OCR model using CNN+RNN with CTC loss\n",
        "def build_ocr_model(img_height, img_width, num_classes):\n",
        "    input_img = layers.Input(shape=(img_height, img_width, 1))  # Input image (Grayscale)\n",
        "    cnn_output = build_cnn((img_height, img_width, 1))(input_img)\n",
        "    rnn_output = build_rnn((None, 256))(cnn_output)\n",
        "\n",
        "    # CTC Loss\n",
        "    labels = layers.Input(name='label', shape=[None], dtype='float32')\n",
        "    input_length = layers.Input(name='input_length', shape=[1], dtype='int64')\n",
        "    label_length = layers.Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "    loss_out = layers.Lambda(ctc_loss_lambda_func, output_shape=(1,), name='ctc')([labels, rnn_output])\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_img, labels, input_length, label_length], outputs=loss_out)\n",
        "\n",
        "    # Compile model with Adam optimizer\n",
        "    model.compile(optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Model summary\n",
        "ocr_model = build_ocr_model(img_height, img_width, num_classes)\n",
        "ocr_model.summary()\n",
        "\n",
        "# Generate random data (for illustration purposes)\n",
        "def generate_dummy_data(num_samples):\n",
        "    images = np.random.rand(num_samples, img_height, img_width, 1)  # Random image data\n",
        "    labels = np.random.randint(1, num_classes, (num_samples, 10))  # Random text labels\n",
        "    input_lengths = np.ones((num_samples, 1)) * (img_width // 4 - 2)  # Dummy input lengths\n",
        "    label_lengths = np.ones((num_samples, 1)) * 10  # Dummy label lengths\n",
        "    return images, labels, input_lengths, label_lengths\n",
        "\n",
        "# Generate dummy data\n",
        "num_samples = 100\n",
        "images, labels, input_lengths, label_lengths = generate_dummy_data(num_samples)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2)\n",
        "\n",
        "# Training the model with dummy data\n",
        "ocr_model.fit([X_train, y_train, input_lengths, label_lengths], y_train, epochs=10, batch_size=16)\n"
      ],
      "metadata": {
        "id": "46x7QsqjgltU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace this function to load actual images and labels\n",
        "def load_custom_dataset():\n",
        "    # Load your images and corresponding labels here\n",
        "    pass\n",
        "\n",
        "# Replace the dummy data generation with the actual dataset\n",
        "images, labels, input_lengths, label_lengths = load_custom_dataset()\n",
        "\n",
        "# Train your model\n",
        "ocr_model.fit([images, labels, input_lengths, label_lengths], labels, epochs=100, batch_size=32)\n"
      ],
      "metadata": {
        "id": "-5HTU-Ivgrea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a new image\n",
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load the image in grayscale\n",
        "    img = cv2.resize(img, (img_width, img_height))  # Resize to match the input size\n",
        "    img = np.expand_dims(img, axis=-1)  # Add channel dimension\n",
        "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
        "    return img\n",
        "\n",
        "# Inference: Extract text from new image\n",
        "new_image_path = 'path_to_your_image.png'\n",
        "new_image = preprocess_image(new_image_path)\n",
        "\n",
        "# Predict with the trained model\n",
        "pred = ocr_model.predict([new_image])\n",
        "\n",
        "# Decode the output (using CTC decoding)\n",
        "decoded_text = tf.keras.backend.ctc_decode(pred, input_length=np.ones(pred.shape[0]) * pred.shape[1])[0][0]\n",
        "\n",
        "print(\"Extracted text: \", decoded_text)\n"
      ],
      "metadata": {
        "id": "dGGstLntguKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "id": "eK-4Z4tCjyrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install tesseract-ocr\n",
        "!apt-get install libtesseract-dev\n"
      ],
      "metadata": {
        "id": "jGU3sbt1kNcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RcX_w2ikNXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure tesseract is installed and configured correctly\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'  # Adjust the path to tesseract executable\n",
        "\n",
        "# Function to preprocess the image and extract text\n",
        "def extract_text(image_path):\n",
        "    # Load the image using OpenCV\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Convert to grayscale for better OCR performance\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply Gaussian Blur to reduce image noise\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "    # Use adaptive thresholding to improve text and symbol recognition\n",
        "    processed_img = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                          cv2.THRESH_BINARY, 11, 2)\n",
        "\n",
        "    # Use morphological operations to enhance text areas\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
        "    processed_img = cv2.morphologyEx(processed_img, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    # Enhance contrast and brightness\n",
        "    processed_img = cv2.convertScaleAbs(processed_img, alpha=1.5, beta=0)\n",
        "\n",
        "\n",
        "    # Use pytesseract to extract text with enhanced configuration for symbols\n",
        "    custom_config = r\"--oem 3 --psm 6 -c tessedit_char_whitelist=0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!@#$%^&*()_+-=~`[]{}|\\\\:;\\'<>,./?₹$€¥¢£\"\n",
        "\n",
        "    # Extract text from the image\n",
        "    extracted_text = pytesseract.image_to_string(processed_img, config=custom_config)\n",
        "\n",
        "    # Remove any unwanted extra characters that may have been added\n",
        "    # Keep only printable characters and specified symbols\n",
        "    extracted_text = ''.join(char for char in extracted_text if char.isprintable() or char in {'₹', '$', '€', '¥', '¢', '£'})\n",
        "\n",
        "    return extracted_text\n",
        "\n",
        "# Main function to run the OCR process\n",
        "def main():\n",
        "    # Path to your image\n",
        "    image_path = '/content/1724676987981-AWSInvoice.png'  # Replace with the actual image path\n",
        "\n",
        "    # Print image path for debugging\n",
        "    print(\"Processing image:\", image_path)\n",
        "\n",
        "    # Extract the text\n",
        "    text = extract_text(image_path)\n",
        "\n",
        "    # Print the extracted text\n",
        "    print(\"Extracted Text:\\n\", text)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPV9h5QWjyoW",
        "outputId": "8df21e04-409b-4e87-996b-04001ed0ede6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing image: /content/1724676987981-AWSInvoice.png\n",
            "Extracted Text:\n",
            " amazon. Nn TaxInvoice/BillofSupply/CashMemo~~) (OriginalforRecipient)IRN/QRCode:GyyentaswynreeeoneBEAOSIes a if eeta SeisEe AOSoe PatniReaaeesarneeae SearsSoldBy: BillingAddress;RKWorldinfocomPvt.Ltd. ASPIRESOFTSERVPRIVATELIMITEDAnganBanquethall,Groundfloor,ONYXroom,9PARISHRAMCOMPLEX,202,SECONDFLOOR,Nandanvan-4,,NearPrernatirthDerasar,satellite, 5BRASHMISOCIETY,AhmedabadAhmedabad-380015 AHMEDABAD,GJ,380007Ahmedabad,Gujarat,380015 ININ GSTRegistratlonNo:24AAQCA1169D1ZUState/UTCode:24PANNo:AAECROS64MGSTReglstrationNo:24AAECROS64M1Z9 ShippingAddress;ASPIRESOFTSERVPRIVATELIMITEDFSSAILicenseNo, PuravGandhi11222999000045 DharmkrupaBunglow,Bhatta,PaldiAHMEDABAD,GUJARAT,380007INState/UTCode:24GSTRegistratlonNo:24AAQCA1169D1ZUPlaceofsupply:GJPlaceofdellvery:GUJARATOrderNumber:405-5277972-7380334 InvolceNumber:FAMA-42378OrderDate:20.06.2024 InvolceDetalls;GJ-FAMA-1293787125-2425InvolceDate:20.06.2024st. a Unit Net[Taxfrax]7: fTotatHoad&ShouldersSmooth&SilkyShampoo,715MI!BOBVW7XSGS(BOBVW7XSGS5) %.$32.20]_2_|%864.40]_995_|CGST]277.80_]%1.020.00HSN:33051090eee SGst|%77.80AmountinWords:OneThousandTwentyonlyForRKWorldinfocomPvt.Ltd.:ReAuthorizedSignatoryASSPL-AmaronSelletServioesPvtLad,ARIPL-AmaronRetadInduaPyt.Lid,fonlywhereAmazonRetadinckaPyt.Lidk,tulimentcentetia¢otocatad)Pleasenokethatthasievciceis.not.ademandfotparyment papercte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Easy-OCRA**"
      ],
      "metadata": {
        "id": "BsTQHx2cc5f_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easyocr\n",
        "!pip install torch torchvision torchaudio\n"
      ],
      "metadata": {
        "id": "ciEBj3Wj3hPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import easyocr\n",
        "import cv2\n",
        "import re\n",
        "\n",
        "\n",
        "# Initialize the EasyOCR reader with the desired language\n",
        "reader = easyocr.Reader(['en'],gpu=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNyKRkXocJTx",
        "outputId": "68c62c05-a671-4abc-fedb-d4914972f545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/easyocr/detection.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net.load_state_dict(copyStateDict(torch.load(trained_model, map_location=device)))\n",
            "/usr/local/lib/python3.10/dist-packages/easyocr/recognition.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the image\n",
        "image_path = '/content/1724676987981-AWSInvoice.png'\n",
        "\n",
        "# Read the image using OpenCV\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Perform OCR using EasyOCR\n",
        "results = reader.readtext(image)\n",
        "\n",
        "# Function to clean unwanted symbols from amounts\n",
        "def clean_text(text):\n",
        "    # Remove any unwanted symbols but keep currency signs, numbers, dots, commas, etc.\n",
        "    cleaned_text = re.sub(r'[^\\d\\w\\s,.₹$€¥¢£]', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "# Extract and clean the text from the OCR results\n",
        "extracted_text = \"\"\n",
        "for (bbox, text, prob) in results:\n",
        "    # Cleaning up the text, if necessary\n",
        "    cleaned_text = clean_text(text)\n",
        "    extracted_text += cleaned_text + \"\\n\"\n",
        "\n",
        "# Display the extracted and cleaned text\n",
        "print(\"Extracted Text:\\n\", extracted_text)\n"
      ],
      "metadata": {
        "id": "doekGusnVnre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c67f7ea-fe6d-4a68-900f-70440d13d1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text:\n",
            " amazonin\n",
            "Tax InvoiceBill of SupplyCash Memo\n",
            "Original for Recipient\n",
            "IRNQR Code\n",
            "Sold By\n",
            "Billing Address\n",
            "R K WorldInfocom Pvt. Ltd.\n",
            "ASPIRE SOFTSERV PRIVATE LIMITED\n",
            "Angan Banquet hall, Ground floor, ONYX room_\n",
            "PARISHRAM COMPLEX, 202, SECOND FLOOR\n",
            "Nandanvan 4,, Near Prernatirth Derasar, satellite\n",
            "SB RASHMI SOCIETY, Ahmedabad\n",
            "Ahmedabad\n",
            "380015\n",
            "AHMEDABAD, GJ, 380007\n",
            "Ahmedabad, Gujarat, 380015\n",
            "IN\n",
            "IN\n",
            "GST Registration No 24AAQCAII6IDIZU\n",
            "StateUT Code 24\n",
            "PAN No AAECROS6AM\n",
            "GST Registration No 24AAECRO564M1Z9\n",
            "Shipping Address\n",
            "ASPIRE SOFTSERV PRIVATE LIMITED\n",
            "FSSAI License No.\n",
            "Purav Gandhi\n",
            "11222999000045\n",
            "Dharmkrupa Bunglow Bhatta, Paldi\n",
            "AHMEDABAD GUJARAT, 380007\n",
            "IN\n",
            "StatelUT Code 24\n",
            "GST Registration No 24AAQCAI16IDIZU\n",
            "Place of supply GJ\n",
            "Place of delivery GUJARAT\n",
            "Order Number 40552779727380334\n",
            "Invoice Number\n",
            "FAMA42378\n",
            "Order Date 20.06.2024\n",
            "Invoice Details\n",
            "GJFAMA12937871252425\n",
            "Invoice Date\n",
            "20.06.2024\n",
            "SI.\n",
            "Unit\n",
            "Net\n",
            "Tax\n",
            "Tax\n",
            "Tax\n",
            "Total\n",
            "Description\n",
            "Qtyl\n",
            "No\n",
            "Price\n",
            "Amount Rate\n",
            "Type\n",
            "Amount Amount\n",
            "Head  Shoulders Smooth  Silky Shampoo, 715MI\n",
            "BOBVWZXSG5\n",
            "BOBVWZXSGS\n",
            "13432.20\n",
            "R864.40\n",
            "90\n",
            "CGST\n",
            "77.80\n",
            "1,020.00\n",
            "HSN33051090\n",
            "SGST\n",
            "77.80\n",
            "TOTAL\n",
            "155.601.020.00\n",
            "Amount in Words\n",
            "One Thousand Twenty only\n",
            "For R K Worldlnfocom Pvt Ltd\n",
            "Authorized Signatory\n",
            "ASSPLAmazon Seller Services Pvt. Ltd , ARIPLAmazon Retail India Pvt Ltd. only where Amazon Retail India Pvt Ltd. fulfillment center\n",
            "colocated\n",
            "Please note that this invoice\n",
            "not\n",
            "demand for payment\n",
            "of 2\n",
            "Page\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tesseract OCR**"
      ],
      "metadata": {
        "id": "QMSH6OXDgL22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n",
        "!sudo apt install tesseract-ocr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqF5aTs8gV0k",
        "outputId": "a5bede70-621e-45d3-b465-74a7ce3cd589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (5,207 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123597 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Ensure Tesseract is installed and configured correctly\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"\n",
        "    Preprocess the image to enhance OCR accuracy.\n",
        "    Converts the image to grayscale, applies Gaussian blur, and adaptive thresholding.\n",
        "    \"\"\"\n",
        "    if not os.path.isfile(image_path):\n",
        "        raise FileNotFoundError(f\"The file at {image_path} does not exist.\")\n",
        "\n",
        "    # Load the image using OpenCV\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Failed to load image at {image_path}.\")\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply Gaussian blur to reduce noise\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "\n",
        "    # Apply adaptive thresholding to binarize the image\n",
        "    processed_img = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                          cv2.THRESH_BINARY, 11, 2)\n",
        "    return processed_img\n",
        "\n",
        "def extract_text(image):\n",
        "    \"\"\"\n",
        "    Extract text from the preprocessed image using Tesseract OCR.\n",
        "    \"\"\"\n",
        "    # Use pytesseract to extract text from image\n",
        "    # Custom configuration to improve OCR accuracy\n",
        "    custom_config = r'--oem 3 --psm 6'\n",
        "\n",
        "    # Extract text\n",
        "    text = pytesseract.image_to_string(image, config=custom_config)\n",
        "\n",
        "    return text\n",
        "\n",
        "def main():\n",
        "    # Path to your image\n",
        "    image_path = '/content/1724676987981-AWSInvoice.png'  # Replace with your image path\n",
        "\n",
        "    try:\n",
        "        # Preprocess the image\n",
        "        processed_image = preprocess_image(image_path)\n",
        "\n",
        "        # Extract text from the preprocessed image\n",
        "        extracted_text = extract_text(processed_image)\n",
        "\n",
        "        # Print the extracted text\n",
        "        print(\"Extracted Text:\\n\", extracted_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0UMkL3ggRas",
        "outputId": "cbaefc21-62ae-4396-fe71-57556c5059a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text:\n",
            " a iv a 20 ni | nN Tax Invoice/Bill of Supply/Cash Memo\n",
            "~eeee) (Original for Recipient)\n",
            "IRN/QR Code: Opuraae oe seen\n",
            "uaa\n",
            "SUES ARATE\n",
            "DAN ae\n",
            "Be Ae Loe!\n",
            "Sold By: Billing Address ;\n",
            "R K Worldinfocom Pvt. Ltd. ASPIRE SOFTSERV PRIVATE LIMITED\n",
            "* angan Banquet hall, Ground floor, ONYX room, © PARISHRAM COMPLEX, 202, SECOND FLOOR,\n",
            "Nandanvan -4,, Near Prernatirth Derasar, satellite, 5B RASHMI SOCIETY, Ahmedabad\n",
            "Ahmedabad - 380015 AHMEDABAD, GJ, 380007\n",
            "Ahmedabad, Gujarat, 380015 IN\n",
            "IN GST Registration No: 24AAQCA1169D1ZU\n",
            "State/UT Code: 24\n",
            "PAN No: AAECROS64M\n",
            "GST Reglstration No: 24AAECRO564M1Z9 Shipping Address :\n",
            "ASPIRE SOFTSERV PRIVATE LIMITED\n",
            "FSSAI License No. Purav Gandhi\n",
            "11222998000045 Dharmkrupa Bunglow, Bhatta, Paldi\n",
            "AHMEDABAD, GUJARAT, 380007\n",
            "IN\n",
            "State/UT Code: 24\n",
            "GST Registration No: 24AAQCA1169D1ZU\n",
            "Place of supply: GJ\n",
            "Place of dellvery: GUJARAT\n",
            "Order Number: 405-5277972-7380334 Involce Number : FAMA-42378\n",
            "Order Date: 20.06.2024 Involce Detalls : GJ-FAMA-1293787 125-2425\n",
            "Invoice Date ; 20.06.2024\n",
            "a ae Net Tax Tax |Tax Total\n",
            "fic [eserotion ice [Amounts [type Amountmaunt\n",
            "Head & Shoulders Smooth & Silky Shampoo, 715M!\n",
            "IBOBVW7XSGS ( BOBVW7XSGS5 ) %.432.20] 2 [3864.40] 9% |CGST] %77.80 ]%1,020.00\n",
            "HSN:33051090\n",
            "9% |SGST| %77.80\n",
            "Amount in Words:\n",
            "One Thousand Twenty only\n",
            "For R K Worldinfocom Pvt. Ltd.:\n",
            "ee\n",
            "Authorized Signatory\n",
            "“ASSPL-Amazon Sollet Servioes Pvt Lad, ARIPL-Amazon Retad Incks Prt. Lick, (only where Amazon Rectal inca Pyt. Lick, fulliment conter is cotocatod)\n",
            "Please note that thas invoice is not a demand lor payment Page sofa\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDJdINmx2_Fv",
        "outputId": "f62a8d23-7ad5-4fc4-f34c-2b41ebd773b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.26.4)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wy78LxT34Oq",
        "outputId": "3840556c-89fb-447b-a471-acd7ca14e871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 2s (2,179 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123597 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pytesseract\n",
        "import cv2\n",
        "\n",
        "# Function to extract text from an image using pytesseract\n",
        "def extract_text_from_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Binarization for better OCR accuracy\n",
        "    _, binary_img = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Extract text using pytesseract\n",
        "    text = pytesseract.image_to_string(binary_img)\n",
        "    return text\n",
        "\n",
        "# Function to create regex patterns for each section\n",
        "def create_regex_patterns():\n",
        "    patterns = {\n",
        "        'Name': r'\\b(?:Mr\\.|Ms\\.|Dr\\.|Prof\\.)?\\s*([A-Z][a-z]+(?:\\s[A-Z][a-z]+)+)\\b',  # Name pattern\n",
        "        'Phone': r'\\+?\\d[\\d\\s\\-\\(\\)]{8,15}',  # Phone number pattern\n",
        "        'Email': r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+',  # Email pattern\n",
        "        'LinkedIn': r'(https?://)?(www\\.)?linkedin\\.com/in/[\\w-]+',  # LinkedIn URL pattern\n",
        "        'Skills': r'Skills[:\\s]+([\\w\\s,]+)',  # Skills section\n",
        "        'Education': r'Education[:\\s]+([\\w\\s,]+(?:\\d{4})?[\\w\\s,]*)',  # Education section\n",
        "        'Experience': r'(Experience[:\\s]+)([\\w\\s,]+\\d{4}\\s*-\\s*\\d{4}[\\w\\s,.]*)',  # Experience section\n",
        "        'Projects': r'Projects[:\\s]+([\\w\\s,.]+)',  # Projects section\n",
        "        'Achievements': r'Achievements[:\\s]+([\\w\\s,.]+)',  # Achievements section\n",
        "        'Location': r'(Location|Address)[:\\s]+([A-Za-z0-9,\\s]+)'  # Address/Location section\n",
        "    }\n",
        "    return patterns\n",
        "\n",
        "# Function to extract sections based on regex patterns\n",
        "def extract_details_using_regex(text, patterns):\n",
        "    details = {}\n",
        "\n",
        "    for label, pattern in patterns.items():\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "        if match:\n",
        "            details[label] = match.group(0)  # Full matched string\n",
        "        else:\n",
        "            details[label] = 'Not Found'\n",
        "\n",
        "    return details\n",
        "\n",
        "# Main function to extract details from the resume image\n",
        "def extract_resume_details_from_image(image_path):\n",
        "    patterns = create_regex_patterns()\n",
        "    text = extract_text_from_image(image_path)\n",
        "\n",
        "    # Extract details using regex patterns\n",
        "    details = extract_details_using_regex(text, patterns)\n",
        "\n",
        "    return details\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = '/content/i1.jpg'\n",
        "\n",
        "    # Extract details from the resume image\n",
        "    resume_details = extract_resume_details_from_image(image_path)\n",
        "\n",
        "    # Print the extracted details\n",
        "    for label, content in resume_details.items():\n",
        "        print(f\"{label}: {content if content else 'Not Found'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11j_cj1f2_DJ",
        "outputId": "56350d93-64d2-4315-f773-3d961ff06524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: MEGHANSHU KUMRAWAT\n",
            "Patel colony\n",
            "Phone: 6265149219 \n",
            "Email: 6@gmail.com\n",
            "LinkedIn: Not Found\n",
            "Skills: skills to a dynamic and\n",
            "innovative team\n",
            "Education: EDUCATION\n",
            "Bachelor of Science \n",
            "Experience: Not Found\n",
            "Projects: Not Found\n",
            "Achievements: Not Found\n",
            "Location: Not Found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYJ1WJHYFCMk",
        "outputId": "c09eae39-e6b8-4f0c-e6db-4c83f19c2054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load annotated data\n",
        "with open('/content/annotated_data.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "texts = []\n",
        "labels = []\n",
        "for item in data:\n",
        "    # Access the value of the 'text' key in the item dictionary\n",
        "    text = item['text']\n",
        "    label = item['label']\n",
        "    texts.append(text)\n",
        "    labels.append(label)\n",
        "\n",
        "# Flatten the lists\n",
        "texts_flat = [word for sublist in texts for word in sublist]\n",
        "labels_flat = [label for sublist in labels for label in sublist]\n",
        "\n",
        "# Tokenize text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts_flat)\n",
        "X_seq = tokenizer.texts_to_sequences(texts_flat)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(labels_flat)\n",
        "y_encoded = y_encoded.reshape(len(labels), -1)\n",
        "\n",
        "# Pad sequences\n",
        "X_padded = pad_sequences(X_seq, padding='post')\n",
        "y_padded = pad_sequences(y_encoded, padding='post')\n",
        "\n",
        "# Convert to one-hot encoding\n",
        "num_labels = len(label_encoder.classes_)\n",
        "y_padded = to_categorical(y_padded, num_classes=num_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "7JaWCdA12_AX",
        "outputId": "88216f98-3472-4ef1-82e8-0945ddf03593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'label'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-cc386d4da855>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Access the value of the 'text' key in the item dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'label'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pytesseract\n",
        "import re\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    _, thresh_img = cv2.threshold(gray_img, 127, 255, cv2.THRESH_BINARY)\n",
        "    denoised_img = cv2.fastNlMeansDenoising(thresh_img, None, 30, 7, 21)\n",
        "    return denoised_img\n",
        "\n",
        "def extract_text(image):\n",
        "    custom_config = r'--oem 3 --psm 6'\n",
        "    text = pytesseract.image_to_string(image, config=custom_config)\n",
        "    return text\n",
        "\n",
        "def extract_section(text, section_name):\n",
        "    pattern = rf'{section_name}\\s*[:\\-]*\\s*(.*?)(?=\\n[A-Z][a-z]|$)'\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    return match.group(1).strip() if match else f'{section_name} not found'\n",
        "\n",
        "def main(image_path):\n",
        "    preprocessed_image = preprocess_image(image_path)\n",
        "    extracted_text = extract_text(preprocessed_image)\n",
        "\n",
        "    # Print entire extracted text for debugging\n",
        "    print(\"Extracted Text:\\n\", extracted_text)\n",
        "\n",
        "    sections = ['Skills', 'Education', 'Experience', 'Projects', 'Achievements']\n",
        "    for section in sections:\n",
        "        section_text = extract_section(extracted_text, section)\n",
        "        print(f'{section}:\\n{section_text}\\n')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main('/content/Data Scientist Resume Example.jpeg')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgzuAN0B2-9q",
        "outputId": "0c2e43dd-6bb8-4746-ebf5-6e8c1226de79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Text:\n",
            " 88 Hendford Hill. London 822 0GX, United Kingdom |078 3515 0056 | emilysaavedraggmeil.com\n",
            "2 Profile @ Education\n",
            "Highly accurate and experienced Data Scientist IB Diploma Programme 09/2010 - 05/2012\n",
            "adept at collecting, analyzing, and interpreting The international School Estepona, €4 Paraiso\n",
            "large datasets, develo ping new forecasting Spain\n",
            "madels, and performing data management tasks.\n",
            "Possessing an extensive analytical skills, strong Graduated with Distinction (Grade | - A/excetlent\n",
            "attention fa detail, and a significant ability to equivatent in all 6 subjects)\n",
            "workin team environments, Emnly is presently\n",
            "looking for a Data Scientist position with a & Skills\n",
            "forward-moving company\n",
            "Languages\n",
            "m@ Work experience Spanish Native\n",
            "English Full\n",
            "SpyBiotech, inc. 09/2017 - 02/2018 Franch Limited\n",
            "Data Scientist, London, United Kingdom |\n",
            "© Assisted in scientific research on BNA ctoning Computer/Data A Skills\n",
            "and analyzed the results. Microsoft Office EE\n",
            "+ Collected, studied, and interpreted large £45 SQL Server ———\n",
            "datasets: conducted reports: performed Sisense Zoho Analytics Se —\n",
            "accurate, successful data management GoodData, Qlik Sense\n",
            "* Developed and implemented new forecasting\n",
            "meclels which increased company Interpersonal Skills\n",
            "productivity and efficiency. Accuracy a\n",
            "‘« Participated in manthty meetings with Analytical Skills a\n",
            "executives, provided infarmatian an the a\n",
            "progress way\n",
            "Multitasking ———\n",
            "Date Scientist 07/2016 - 09/2017\n",
            "GL. Inc.. London, United Kingdom Volunteering\n",
            "+ Collected, analyzed and interpreted raw data\n",
            "from various websites, English Tutor 06/2014 - 08/2014\n",
            "* Collaborated with the Operations and Co Overseas, Suva, Fiji\n",
            "Technology Department on the development\n",
            "of new automated data\n",
            "management/analysis software which & Certificates\n",
            "increased the overall productivity and cut\n",
            "es Professional Certificate in 10/2016\n",
            "+ Maintained and managed companys MS SQL Do aati\n",
            "Increased the accuracy of forecasting The Chartered Institute for IT\n",
            "software from 80% to 95%.\n",
            "Driving Licence 03/2012\n",
            "@ Education Driving School\n",
            "Mathematics and 09/2012 - 05/2016 A\n",
            "Statistics “® Hobbies\n",
            "University of Chichester, Chichester United\n",
            "Kingdom .\n",
            "fsCoss Honours aA Ht @®\n",
            "Glubs anc societies: ‘Club Jeties: Qusiness Club, Golf Club. Exploring Gettinglostin Every kind of\n",
            "ding Cut distantiands a good book sport\n",
            "\f\n",
            "Skills:\n",
            "forward-moving company\n",
            "\n",
            "Education:\n",
            "Highly accurate and experienced Data Scientist IB Diploma Programme 09/2010 - 05/2012\n",
            "adept at collecting, analyzing, and interpreting The international School Estepona, €4 Paraiso\n",
            "large datasets, develo ping new forecasting Spain\n",
            "madels, and performing data management tasks.\n",
            "\n",
            "Experience:\n",
            "Experience not found\n",
            "\n",
            "Projects:\n",
            "Projects not found\n",
            "\n",
            "Achievements:\n",
            "Achievements not found\n",
            "\n"
          ]
        }
      ]
    }
  ]
}